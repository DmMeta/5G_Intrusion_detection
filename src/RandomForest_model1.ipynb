{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#General Imports\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from math import ceil\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rnd\n",
        "from time import perf_counter as time\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-OhDfNrcCby",
        "outputId": "7dc68b15-9cd0-4826-9658-ffac40482d19"
      },
      "outputs": [],
      "source": [
        "dataset_fpath = '../data/BTS1_BTS2_fields_preserved.zip'\n",
        "nidd_dataset = pd.read_csv(dataset_fpath, compression = 'zip', low_memory=False)\n",
        "\n",
        "#Attack Type for Multi-Class Classification or Label for Binary Classification\n",
        "TARGET = 'Label'\n",
        "DROPPED_TARGET = \"Label\" if TARGET == \"Attack Type\" else \"Attack Type\"\n",
        "\n",
        "useless_features = ['Attack Tool', DROPPED_TARGET, 'Dport', 'Sport', 'SrcAddr', 'DstAddr',\"Unnamed: 0\"]\n",
        "nidd_dataset = nidd_dataset.drop(useless_features, axis=1)\n",
        "print(f\"Original shape of data: {nidd_dataset.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropping Features with the following properties:\n",
        "    - Having NaN values above 95% of their total values\n",
        "    - Having Zero values above 95% of their total values\n",
        "    - Having constant values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_exMFEXP5_Qp",
        "outputId": "48b416ae-af67-4ef9-d562-d406db797031"
      },
      "outputs": [],
      "source": [
        "features_dropped = {\"Nan columns\": [], \"Zero columns\": [], \"Constant columns\": [], \"General columns\": useless_features}\n",
        "preprocessing_d = {\"Time <Dropping features based on a threshold `bad` values>\": 0.,\n",
        "                   \"Filling NaNs\": 0.,\n",
        "                   \"Time <Encoding categorical features>\": 0.,\n",
        "                   \"Time <Feature selection>\": 0.}\n",
        "\n",
        "\n",
        "t0 = time()\n",
        "\n",
        "threshold_nans = 0.95 * nidd_dataset.shape[0]\n",
        "columns_Nan_to_drop = nidd_dataset.columns[nidd_dataset.isna().sum() > threshold_nans]\n",
        "features_dropped[\"Nan columns\"].extend(columns_Nan_to_drop)\n",
        "nidd_dataset_cleaned_nan  = nidd_dataset.drop(columns=columns_Nan_to_drop)\n",
        "# eq nidd_dataset_cleaned_nan = nidd_dataset.dropna(thresh = ceil(1 - threshold_nans = 0.95 * nidd_dataset.shape[0]), axis = 1)\n",
        "print(f\"After dropping NaN columns: {nidd_dataset_cleaned_nan.shape}\")\n",
        "\n",
        "threshold_zeros = 0.95 * nidd_dataset.shape[0]\n",
        "zero_counts = nidd_dataset_cleaned_nan.apply(lambda col: (col == 0).sum())\n",
        "columns_zeros_to_drop = zero_counts[zero_counts > threshold_zeros].index\n",
        "features_dropped[\"Zero columns\"].extend(columns_zeros_to_drop)\n",
        "nidd_dataset_cleaned_zeros = nidd_dataset_cleaned_nan.drop(columns_zeros_to_drop, axis=1)\n",
        "print(f\"After dropping NaN & Zero columns: {nidd_dataset_cleaned_zeros.shape}\")\n",
        "\n",
        "constant_columns = nidd_dataset_cleaned_zeros.columns[nidd_dataset_cleaned_zeros.nunique() == 1]\n",
        "features_dropped[\"Constant columns\"].extend(constant_columns)\n",
        "nidd_dataset_cleaned = nidd_dataset_cleaned_zeros.drop(columns=constant_columns)\n",
        "print(f\"After dropping NaN & Zero & Constant columns: {nidd_dataset_cleaned.shape}\")\n",
        "\n",
        "t1 = time()\n",
        "\n",
        "\n",
        "preprocessing_d[\"Time <Dropping features based on a threshold `bad` values>\"] = t1 - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replacing NaN values & label encoding categorical features:\n",
        "    - Numerical features with their mean\n",
        "    - Categorical features with the most frequent category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ztjFZv5ayd"
      },
      "outputs": [],
      "source": [
        "# Getting columns out of their respective dtypes (numeric and categorical)\n",
        "\n",
        "t0 = time()\n",
        "\n",
        "numeric_cols = nidd_dataset_cleaned.select_dtypes(include='number').columns\n",
        "categorical_cols = nidd_dataset_cleaned.select_dtypes(include='object').columns\n",
        "numeric_cols_mean = nidd_dataset_cleaned[numeric_cols].mean()\n",
        "nidd_dataset_cleaned[numeric_cols] = nidd_dataset_cleaned[numeric_cols].fillna(numeric_cols_mean)\n",
        "\n",
        "#iloc[0] is used to get the first element of the series in case there is more than one category that produces \n",
        "# the same mode.\n",
        "categorical_cols_mode = nidd_dataset_cleaned[categorical_cols].mode().iloc[0]\n",
        "nidd_dataset_cleaned[categorical_cols] = nidd_dataset_cleaned[categorical_cols].fillna(categorical_cols_mode)\n",
        "\n",
        "t1 = time()\n",
        "\n",
        "\n",
        "preprocessing_d[\"Time <Filling NaNs>\"] = t1 - t0\n",
        "\n",
        "\n",
        "t0 = time()\n",
        "# Encoding categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "for column in categorical_cols:\n",
        "    nidd_dataset_cleaned[column] = label_encoder.fit_transform(nidd_dataset_cleaned[column])\n",
        "\n",
        "t1 = time()\n",
        "preprocessing_d[\"Time <Encoding categorical features>\"] = t1 - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating the Pearson Correlation matrix and plotting its Heatmap\n",
        "    We find the pairwise corellation for every single pair of features. We also consider absolute values\n",
        "    so as to include high negative correlation as well. Based on a threshold(0.9 here) we remove one feature\n",
        "    from each pair(the one having the lowest correlation )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW1ogo214c-8",
        "outputId": "f40ae2c0-b742-4343-e83e-f02f874d01f9"
      },
      "outputs": [],
      "source": [
        "t0 = time()\n",
        "\n",
        "target_col = nidd_dataset_cleaned.columns[-1]\n",
        "correlation_matrix = nidd_dataset_cleaned.corr(method = 'pearson')\n",
        "corr_threshold = 0.9\n",
        "# high_corr_pairs = (correlation_matrix.abs() > corr_threshold) & (correlation_matrix.abs() < 1)\n",
        "high_corr_pairs = correlation_matrix.iloc[:,:-1].abs() > corr_threshold\n",
        "# print(high_corr_pairs)\n",
        "features_to_drop = set() # avoid duplicate entries\n",
        "\n",
        "for feature_on_col in high_corr_pairs.columns:\n",
        "    for feature_in_row in high_corr_pairs.index[high_corr_pairs[feature_on_col]]:\n",
        "        if feature_on_col != feature_in_row:  # avoid comparisons between same pairs\n",
        "            # getting the correlation values for each feature against target variable\n",
        "            feature_in_row_cor_val = correlation_matrix.loc[feature_in_row, target_col]\n",
        "            feature_on_col_cor_val = correlation_matrix.loc[feature_on_col, target_col]\n",
        "            \n",
        "            same_corellation_sign = feature_in_row_cor_val * feature_on_col_cor_val > 0\n",
        "            \n",
        "            if same_corellation_sign:\n",
        "                if abs(feature_in_row_cor_val) > abs(feature_on_col_cor_val):\n",
        "                    features_to_drop.add(feature_on_col)\n",
        "                else:\n",
        "                    features_to_drop.add(feature_in_row)\n",
        "\n",
        "\n",
        "nidd_dataset_filtered = nidd_dataset_cleaned.drop(columns=list(features_to_drop))\n",
        "t1 = time()\n",
        "print(f\"Final shape of cleaned DataFrame after removing correlated features based on Pearson method: {nidd_dataset_filtered.shape}\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', \n",
        "            fmt='.2f', xticklabels=False, yticklabels=False,\n",
        "            linewidths=.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Anova F-test to calculate a score that quantifies changes in mean and variance against the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5bBeev6dIb4",
        "outputId": "80a07001-3fe2-4e16-d53d-d6b48aff85d4"
      },
      "outputs": [],
      "source": [
        "X = nidd_dataset_filtered.drop(columns=[target_col])\n",
        "y = nidd_dataset_filtered[target_col]\n",
        "\n",
        "t2 = time()\n",
        "f_scores, _ = f_classif(X, y)\n",
        "results_df = pd.DataFrame({'Feature': X.columns, 'F-Score': f_scores})\n",
        "results_df = results_df.sort_values(by='F-Score', ascending=False)\n",
        "t3 = time()\n",
        "preprocessing_d[\"Time <Feature selection>\"] = t1-t0 + t3-t2\n",
        "\n",
        "top_10_features = results_df.head(10)\n",
        "print(\"Top 10 Features based on ANOVA F-scores\")\n",
        "for index, row in top_10_features.iterrows():\n",
        "    print(f\"{row['Feature']}: {row['F-Score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applying Z-normalization to the selected features, training a RandomForest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "lA6QfmD8gNyE",
        "outputId": "c472ce9a-a7a2-4516-bff2-99c9edec9e99"
      },
      "outputs": [],
      "source": [
        "\n",
        "top_10_features_cols = top_10_features['Feature'].tolist()\n",
        "target = nidd_dataset_filtered[TARGET]\n",
        "features = nidd_dataset[top_10_features_cols]\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=rnd.seed(42)) \n",
        "\n",
        "numeric_features = list(set(numeric_cols) & set(top_10_features_cols))\n",
        "categorical_features = list(set(categorical_cols) & set(top_10_features_cols))\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')), \n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  \n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "        \n",
        "    ])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "predictions = pipeline.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting Confusion Matrix and classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "target_categories = set(nidd_dataset[TARGET])\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(conf_matrix, cmap=plt.cm.Blues, interpolation='nearest')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(ticks=np.arange(len(target_categories)), labels=target_categories, rotation=45)\n",
        "plt.yticks(ticks=np.arange(len(target_categories)), labels=target_categories)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(conf_matrix[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2 else \"black\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "class_report = classification_report(y_test, predictions, target_names=target_categories , digits=6)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "for key, time in preprocessing_d.items():\n",
        "    print(f\"{key}: {time:.4f} seconds\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
